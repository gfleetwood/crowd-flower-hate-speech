{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Looking At Hate Speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the beauty of the internet age comes with its fair share of ugliness as well. In recent months, this dark side of the web that lives on Twitter has been in the spotlight as a component of the wider deluge of pieces on that company's future. As great as it is as a repository of conversation on almost every topic, it's the bad eggs that seem to define Twitter recently. As seen from Microsoft's ill-fated AI experiment on Twitter, hate speech is a frequent vehicle of choice in the platform's darker flight paths. \n",
    "\n",
    "From a Natural Language Processing perspective, identifying hate speech is an intriguing problem as the notion of what does or doesn't qualify - for better or for worse - can vary. Firstly, though, one has to get some data - some labeled data to be more specific. You could go the route of collecting and then labeling it yourself, but that will be: 1) quite time consuming and 2) incredibly biased towards your own perspective. How then to be time efficient and less biased?\n",
    "\n",
    "The answer is to crowd source it, but even that is made trivial by the fact that CrowdFlower already has such a dataset available for [free](http://www.crowdflower.com/data-for-everyone/), one of which concerns hate speech on Twitter. Here's the description:\n",
    "\n",
    ">Hate speech identification\n",
    "\n",
    ">Contributors viewed short text and identified if it a) contained hate speech, b) was offensive but without hate speech, or c) >was not offensive at all. Contains nearly 15K rows with three contributor judgments per text string.\n",
    "\n",
    "With that obstacle avoided now all thatâ€™s left is to build a model to identify hate speech. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 14509 data points.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>_golden</th>\n",
       "      <th>_unit_state</th>\n",
       "      <th>_trusted_judgments</th>\n",
       "      <th>_last_judgment_at</th>\n",
       "      <th>does_this_tweet_contain_hate_speech</th>\n",
       "      <th>does_this_tweet_contain_hate_speech:confidence</th>\n",
       "      <th>_created_at</th>\n",
       "      <th>orig__golden</th>\n",
       "      <th>orig__last_judgment_at</th>\n",
       "      <th>orig__trusted_judgments</th>\n",
       "      <th>orig__unit_id</th>\n",
       "      <th>orig__unit_state</th>\n",
       "      <th>_updated_at</th>\n",
       "      <th>orig_does_this_tweet_contain_hate_speech</th>\n",
       "      <th>does_this_tweet_contain_hate_speech_gold</th>\n",
       "      <th>does_this_tweet_contain_hate_speech_gold_reason</th>\n",
       "      <th>does_this_tweet_contain_hate_speechconfidence</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>853718217</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>86</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The tweet uses offensive language but not hate...</td>\n",
       "      <td>0.6013</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>615561535</td>\n",
       "      <td>golden</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The tweet contains hate speech</td>\n",
       "      <td>The tweet contains hate speech\\nThe tweet uses...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1666196150</td>\n",
       "      <td>Warning: penny boards will make you a faggot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>853718218</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>92</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The tweet contains hate speech</td>\n",
       "      <td>0.7227</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>615561723</td>\n",
       "      <td>golden</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The tweet contains hate speech</td>\n",
       "      <td>The tweet contains hate speech\\nThe tweet uses...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>429512078</td>\n",
       "      <td>Fuck dykes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>853718219</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>86</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The tweet contains hate speech</td>\n",
       "      <td>0.5229</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>615562039</td>\n",
       "      <td>golden</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The tweet contains hate speech</td>\n",
       "      <td>The tweet contains hate speech\\nThe tweet uses...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>395623778</td>\n",
       "      <td>@sizzurp__ @ILIKECATS74 @yoPapi_chulo @brandon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>853718220</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>98</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The tweet contains hate speech</td>\n",
       "      <td>0.5184</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>615562068</td>\n",
       "      <td>golden</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The tweet contains hate speech</td>\n",
       "      <td>The tweet contains hate speech\\nThe tweet uses...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>497514685</td>\n",
       "      <td>\"@jayswaggkillah: \"@JacklynAnnn: @jayswaggkill...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>853718221</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>88</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The tweet uses offensive language but not hate...</td>\n",
       "      <td>0.5185</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>615562488</td>\n",
       "      <td>golden</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The tweet contains hate speech</td>\n",
       "      <td>The tweet contains hate speech\\nThe tweet uses...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>588923553</td>\n",
       "      <td>@Zhugstubble You heard me bitch but any way I'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    _unit_id _golden _unit_state  _trusted_judgments _last_judgment_at  \\\n",
       "0  853718217    True      golden                  86               NaN   \n",
       "1  853718218    True      golden                  92               NaN   \n",
       "2  853718219    True      golden                  86               NaN   \n",
       "3  853718220    True      golden                  98               NaN   \n",
       "4  853718221    True      golden                  88               NaN   \n",
       "\n",
       "                 does_this_tweet_contain_hate_speech  \\\n",
       "0  The tweet uses offensive language but not hate...   \n",
       "1                     The tweet contains hate speech   \n",
       "2                     The tweet contains hate speech   \n",
       "3                     The tweet contains hate speech   \n",
       "4  The tweet uses offensive language but not hate...   \n",
       "\n",
       "   does_this_tweet_contain_hate_speech:confidence  _created_at orig__golden  \\\n",
       "0                                          0.6013          NaN         True   \n",
       "1                                          0.7227          NaN         True   \n",
       "2                                          0.5229          NaN         True   \n",
       "3                                          0.5184          NaN         True   \n",
       "4                                          0.5185          NaN         True   \n",
       "\n",
       "   orig__last_judgment_at  orig__trusted_judgments  orig__unit_id  \\\n",
       "0                     NaN                        0      615561535   \n",
       "1                     NaN                        0      615561723   \n",
       "2                     NaN                        0      615562039   \n",
       "3                     NaN                        0      615562068   \n",
       "4                     NaN                        0      615562488   \n",
       "\n",
       "  orig__unit_state  _updated_at orig_does_this_tweet_contain_hate_speech  \\\n",
       "0           golden          NaN           The tweet contains hate speech   \n",
       "1           golden          NaN           The tweet contains hate speech   \n",
       "2           golden          NaN           The tweet contains hate speech   \n",
       "3           golden          NaN           The tweet contains hate speech   \n",
       "4           golden          NaN           The tweet contains hate speech   \n",
       "\n",
       "            does_this_tweet_contain_hate_speech_gold  \\\n",
       "0  The tweet contains hate speech\\nThe tweet uses...   \n",
       "1  The tweet contains hate speech\\nThe tweet uses...   \n",
       "2  The tweet contains hate speech\\nThe tweet uses...   \n",
       "3  The tweet contains hate speech\\nThe tweet uses...   \n",
       "4  The tweet contains hate speech\\nThe tweet uses...   \n",
       "\n",
       "   does_this_tweet_contain_hate_speech_gold_reason  \\\n",
       "0                                              NaN   \n",
       "1                                              NaN   \n",
       "2                                              NaN   \n",
       "3                                              NaN   \n",
       "4                                              NaN   \n",
       "\n",
       "   does_this_tweet_contain_hate_speechconfidence    tweet_id  \\\n",
       "0                                              1  1666196150   \n",
       "1                                              1   429512078   \n",
       "2                                              1   395623778   \n",
       "3                                              1   497514685   \n",
       "4                                              1   588923553   \n",
       "\n",
       "                                          tweet_text  \n",
       "0       Warning: penny boards will make you a faggot  \n",
       "1                                         Fuck dykes  \n",
       "2  @sizzurp__ @ILIKECATS74 @yoPapi_chulo @brandon...  \n",
       "3  \"@jayswaggkillah: \"@JacklynAnnn: @jayswaggkill...  \n",
       "4  @Zhugstubble You heard me bitch but any way I'...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "hate_speech = pd.read_csv('../../Desktop/Jupyter/_data/twitter-hate-speech-classifier-DFE-a845520.csv', \n",
    "                          encoding = 'iso-8859-1')\n",
    "print('There are', len(hate_speech), 'data points.')\n",
    "hate_speech.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are exactly 14,509 rows in this dataset and it looks very messy. Unfortunately a code book wasn't available, but some of the columns are easy enough to understand. Let's get a grasp on the missingness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_unit_id                                               0\n",
       "_golden                                                0\n",
       "_unit_state                                            0\n",
       "_trusted_judgments                                     0\n",
       "_last_judgment_at                                     67\n",
       "does_this_tweet_contain_hate_speech                    0\n",
       "does_this_tweet_contain_hate_speech:confidence         0\n",
       "_created_at                                        14509\n",
       "orig__golden                                       14442\n",
       "orig__last_judgment_at                             14509\n",
       "orig__trusted_judgments                            14442\n",
       "orig__unit_id                                      14442\n",
       "orig__unit_state                                   14442\n",
       "_updated_at                                        14509\n",
       "orig_does_this_tweet_contain_hate_speech           14442\n",
       "does_this_tweet_contain_hate_speech_gold           14442\n",
       "does_this_tweet_contain_hate_speech_gold_reason    14509\n",
       "does_this_tweet_contain_hate_speechconfidence      14442\n",
       "tweet_id                                               0\n",
       "tweet_text                                             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hate_speech) - hate_speech.count() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a lot of missing data, but luckily the (seemingly) most relevant columns are all complete. I'll keep the tweets, whether or not it contains hate speech, and the level of confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 _unit_id\n",
      "1 _golden\n",
      "2 _unit_state\n",
      "3 _trusted_judgments\n",
      "4 _last_judgment_at\n",
      "5 does_this_tweet_contain_hate_speech\n",
      "6 does_this_tweet_contain_hate_speech:confidence\n",
      "7 _created_at\n",
      "8 orig__golden\n",
      "9 orig__last_judgment_at\n",
      "10 orig__trusted_judgments\n",
      "11 orig__unit_id\n",
      "12 orig__unit_state\n",
      "13 _updated_at\n",
      "14 orig_does_this_tweet_contain_hate_speech\n",
      "15 does_this_tweet_contain_hate_speech_gold\n",
      "16 does_this_tweet_contain_hate_speech_gold_reason\n",
      "17 does_this_tweet_contain_hate_speechconfidence\n",
      "18 tweet_id\n",
      "19 tweet_text\n"
     ]
    }
   ],
   "source": [
    "for i, col in enumerate(hate_speech.columns):\n",
    "    print(i, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Verdict</th>\n",
       "      <th>Confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Warning: penny boards will make you a faggot</td>\n",
       "      <td>The tweet uses offensive language but not hate...</td>\n",
       "      <td>0.6013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fuck dykes</td>\n",
       "      <td>The tweet contains hate speech</td>\n",
       "      <td>0.7227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@sizzurp__ @ILIKECATS74 @yoPapi_chulo @brandon...</td>\n",
       "      <td>The tweet contains hate speech</td>\n",
       "      <td>0.5229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"@jayswaggkillah: \"@JacklynAnnn: @jayswaggkill...</td>\n",
       "      <td>The tweet contains hate speech</td>\n",
       "      <td>0.5184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@Zhugstubble You heard me bitch but any way I'...</td>\n",
       "      <td>The tweet uses offensive language but not hate...</td>\n",
       "      <td>0.5185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Tweets  \\\n",
       "0       Warning: penny boards will make you a faggot   \n",
       "1                                         Fuck dykes   \n",
       "2  @sizzurp__ @ILIKECATS74 @yoPapi_chulo @brandon...   \n",
       "3  \"@jayswaggkillah: \"@JacklynAnnn: @jayswaggkill...   \n",
       "4  @Zhugstubble You heard me bitch but any way I'...   \n",
       "\n",
       "                                             Verdict  Confidence  \n",
       "0  The tweet uses offensive language but not hate...      0.6013  \n",
       "1                     The tweet contains hate speech      0.7227  \n",
       "2                     The tweet contains hate speech      0.5229  \n",
       "3                     The tweet contains hate speech      0.5184  \n",
       "4  The tweet uses offensive language but not hate...      0.5185  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hate_speech_subset = hate_speech.iloc[:, [19, 5, 6]]\n",
    "hate_speech_subset.columns = ['Tweets', 'Verdict', 'Confidence']\n",
    "hate_speech_subset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Verdict column is the target and has three labels which could be categorical or ordinal depending on how you want to interpret it. For ease of use in different algorithms it's best to encode the values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Verdict</th>\n",
       "      <th>Confidence</th>\n",
       "      <th>Numeric_Verdict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Warning: penny boards will make you a faggot</td>\n",
       "      <td>The tweet uses offensive language but not hate...</td>\n",
       "      <td>0.6013</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fuck dykes</td>\n",
       "      <td>The tweet contains hate speech</td>\n",
       "      <td>0.7227</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@sizzurp__ @ILIKECATS74 @yoPapi_chulo @brandon...</td>\n",
       "      <td>The tweet contains hate speech</td>\n",
       "      <td>0.5229</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"@jayswaggkillah: \"@JacklynAnnn: @jayswaggkill...</td>\n",
       "      <td>The tweet contains hate speech</td>\n",
       "      <td>0.5184</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@Zhugstubble You heard me bitch but any way I'...</td>\n",
       "      <td>The tweet uses offensive language but not hate...</td>\n",
       "      <td>0.5185</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Tweets  \\\n",
       "0       Warning: penny boards will make you a faggot   \n",
       "1                                         Fuck dykes   \n",
       "2  @sizzurp__ @ILIKECATS74 @yoPapi_chulo @brandon...   \n",
       "3  \"@jayswaggkillah: \"@JacklynAnnn: @jayswaggkill...   \n",
       "4  @Zhugstubble You heard me bitch but any way I'...   \n",
       "\n",
       "                                             Verdict  Confidence  \\\n",
       "0  The tweet uses offensive language but not hate...      0.6013   \n",
       "1                     The tweet contains hate speech      0.7227   \n",
       "2                     The tweet contains hate speech      0.5229   \n",
       "3                     The tweet contains hate speech      0.5184   \n",
       "4  The tweet uses offensive language but not hate...      0.5185   \n",
       "\n",
       "   Numeric_Verdict  \n",
       "0                2  \n",
       "1                0  \n",
       "2                0  \n",
       "3                0  \n",
       "4                2  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(list(hate_speech_subset.Verdict.unique()))\n",
    "hate_speech_subset['Numeric_Verdict'] = le.transform(list(hate_speech_subset.Verdict.values))\n",
    "hate_speech_subset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I have numbers representing each label, but it's easy to see the number-label relationship. (As if the snapshot above didn't make it clear.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : The tweet contains hate speech\n",
      "1 : The tweet is not offensive\n",
      "2 : The tweet uses offensive language but not hate speech\n"
     ]
    }
   ],
   "source": [
    "for i, label in enumerate(le.classes_):\n",
    "    print(i, ':', label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Confidence as a variable would be straightforward but processing has to be done on the Tweets to use them as predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re, string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def processTweet(tweet):\n",
    "\n",
    "    tweet = tweet.lower()    \n",
    "    #Remove urls\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', '', tweet)    \n",
    "    #Remove usernames\n",
    "    tweet = re.sub('@[^\\s]+','',tweet)    \n",
    "    #Remove white space\n",
    "    tweet = tweet.strip()    \n",
    "    #Remove hashtags\n",
    "    tweet = re.sub(r'#([^\\s]+)', '', tweet)   \n",
    "    #Remove stopwords\n",
    "    tweet = \" \".join([word for word in tweet.split(' ') if word not in stopwords.words('english')])\n",
    "    #Remove punctuation\n",
    "    tweet = \"\".join(l for l in tweet if l not in string.punctuation)\n",
    "    \n",
    "    return tweet\n",
    "\n",
    "hate_speech_subset['Tweets'] = hate_speech_subset['Tweets'].map(lambda x: processTweet(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now I'm just going to use the tweets as a predictor and use the Confidence later on in another capacity. The lack of a code book really hurts here because it's not clear how the Confidence was determined given that the label was mostly likely a majority vote between three people. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "text = hate_speech_subset['Tweets'].values\n",
    "vectorizer = CountVectorizer(ngram_range = (1, 2))\n",
    "vectorizer.fit(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In theory using the TfidfVectorizer to transform the text into a matrix would be a better approach but it didn't in this case - believe me, I tried. I think it may be a length (of tweets in general) issue.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First the baseline accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.501344\n",
       "2    0.333310\n",
       "0    0.165346\n",
       "dtype: float64"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hate_speech_subset['Numeric_Verdict'].value_counts()/len(hate_speech_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the classification would be 50% accurate if the most frequent class was always chosen as the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14509, 90422)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X = vectorizer.transform(text)\n",
    "y = hate_speech_subset['Numeric_Verdict'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thats a pretty big sparse matrix. Unfortunately dimensionality reduction isn't an option due to its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rfc : 0.963790092822\n",
      "mnb : 0.933921514567\n",
      "lr : 0.963330576234\n",
      "svm : 0.973623747817\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "clfs = {'lr': LogisticRegression(), \n",
    "        'mnb': MultinomialNB(), \n",
    "        'rfc': RandomForestClassifier(), \n",
    "        'svm': SVC(kernel = 'linear', probability = True)}\n",
    "\n",
    "def test_clf(clf_dict, Xtrain, ytrain):\n",
    "    for clf_name, clf in clf_dict.items():\n",
    "        clf.fit(Xtrain, ytrain)\n",
    "        print(clf_name,':', clf.score(Xtrain, ytrain))\n",
    "        \n",
    "test_clf(clfs, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifiers do pretty well with the Random Forest Classifier and the SVC with a linear kernel being the best. (I tried other kernels for the SVC and they didn't perform well.) These also have the most parameters to tune, so I'll stick with them in a bid to get even higher accuracy. Before I move on though, I want to look at the accuracy in a more nuanced way with the SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALEAAAC2CAYAAACWLGx9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAFb9JREFUeJzt3XlcVGXfx/HPDxARccXt1jRcUiQXXMIlEco0TCkrd+1J\nrGyxtCwys9DKUp+6W7R9MevO3NNcMrc7BZfUXEpzKa2eFDNF00IzGLmeP+aAjAsOMMNw8Pd+vXw5\n55pzzvWb8euZa86cRYwxKGVnfr4uQKnC0hAr29MQK9vTECvb0xAr2wvwZeciortGVL4YY+TcNp+G\nGODQidO+LqFAqoSU9nUJBWbX3aqlAi48cNDhhLI9DbGyPQ2xsj0NsbI9DbGyPQ2xsj0NsbI9DbGy\nPQ2xsj0NsbI9DbGyPQ2xsj0NsbI9DbGyPQ2xsj0NsbI9DbGyPa+GWETiRGS3iPwoIiMLu76Hhw6h\nSYM6xLZr5dL+/jtvEn1Nc2LatuS5pNEuzx3Y/yv1aoby1uRXc9oyMjJ4bNgDXNuqKdHXNGfxgvmF\nLc0jJk16jcjmTWnerAmTJr3m63LOc/fdg6lVswaRkc1y2ubMmU3zZk0oHRjAli1bXOafOGE8jcMb\n0uTqxixftsxrdXktxCLiD7wOxAERQD8RaVyYdfYd8D9Mn7vApW1N8iqWfbGI/677htVfb+H+YQ+7\nPD/myZHc0KWrS9urL02gavXqrN28nZRN39KuQ3RhyvKIHTt2MOWD9/l6wya2bP2WxYsWsW/fPl+X\n5WLQnQksWrzEpa1Jk6bMnvMZ0dEdXdp37tzJrNmz+G779yxavISHHhpKVlaWV+ry5pY4CthrjPnF\nGJMJzABuKcwK27bvQIWKFV3aPvrgPYaNSKRUqVIAVKlSNee5JYsWcGVYXRqGh7ssM3Paxwwb8XjO\ndOXKoYUpyyP27N5NVFQbgoKC8Pf3p2NMDPPmfebrslx0iI6mUqVKLm3h4eE0bNjwvHkXLvicvn36\nUqpUKcLCwqhfvwEbN270Sl3eDHEtYH+u6QNWm0f9vG8v69et4aZOHbm1W2e2bdkMwMn0dN547WUe\nG/WUy/wnjh8HYMK4sXTp2I577hzAkSOHPV1Wvl3dpAlr1qRw7NgxTp06xRdfLCb1wAFfl1VgB387\nSK0rrsiZrnVFLQ4eTPVKX94McZGcUus44+DE8eN8sTKZpGfHM2TQAABeHD+OIQ88RHBwsMvZvY4z\nDg6mphLVph3LktfTOqoNzzz1RFGUmqfw8HASE0fSNa4L3bp1pUVkC8SvZH3vFjnvbHuP8OYp+6lA\n7VzTtXFujV28OP65nMftO3Tk2uiYfHXyr5q1uCneOUpp0ao1fn5+HD2axrYtm1i8YB7PJT3JnydO\n4OfnR1BQGQbdPYQywcF0u7kHAN1vuZVP/zM1v6/NKxIGDyZh8GAARo9+kjq16/i4ooKrVbMWB/af\n/SBOPZBKzZoe/yAGvBvib4CrRCQMOAj0AfqdO1PiqKcL1UnXbvGsSV5F+w4d2bf3RzIzMwgNrcL8\nJStz5nlpwjhCQsqRcM+9AHSJu4k1yavo0DGWlNVf0Sg8olA1eMrhw4epVq0av/76K5/Pn8e69Rt8\nXVK+5P7E6x5/M3fcMYCHHxlBamoqe/f+SFRUlFf69VqIjTEOEXkQWAr4Ax8YY3YVZp33Db6D9WvX\n8Mexo7SMqE/ik0n0u2MQjwwdQmy7VgSWCmTS2x9ccj1PPfM8D947mKRRiVSpUpVX33y3MGV5TO/e\nPTl29CilSpVi8utvUr58eV+X5GLggP4kJ68mLS2NumF1SBozlsqVK/Pw8GGkpaVxy83diYxswaLF\nXxAREUGvnr1o1vRqAgICmDz5Da8NJ8SXV4MREaNXACp6dr4C0IUuY1Wyvjmoy5KGWNmehljZnoZY\n2Z6GWNmehljZnoZY2Z6GWNmehljZnoZY2Z6GWNmehljZnoZY2Z6GWNmehljZnoZY2Z6GWNmehljZ\nns9PT3KcseepMkvXbPd1CQV2Y4emvi6hQAL8RU9PUiXTRc92FpF0Ln4BFGOMKV6n4qrL1kVDbIwJ\nKcpClCoot4YTIhItIgnW46oiUte7ZSnlvkuGWETGAiOBUVZTIDDNizUplS/ubIlvBW4GTgIYY1IB\nHWqoYsOdEP9jjMm5OrKIlPViPUrlmzshni0i7wAVRWQIsBJ437tlKeW+S15Q0Bjzooh0Af4CGgJP\nG2OWe70ypdzk7lUxtwNlcO43tu9PVapEcmfvxN3ABuA24HZgg4jc5e3ClHKXO1vix4EWxpijACIS\nCqwHLn0hYKWKgDtf7NKA9FzT6VabUsVCXsdOPGo93ItzCJF9x8JbgO+8XZhS7sprOFEO5xe5fcBP\nnD0Y6HOK6M5ISrkjrwOAxhZhHUoV2CW/2IlINZxf7iJw7mYD56GY13uzMKXc5c4Xu2nAbqAeMBb4\nBeftvZQqFtwJcagx5n0gwxiz2hiTAOhWWBUb7uwnzrD+PiQi3XHeWLFSHvPnEJEpQDfgsDGmSE7s\n2rNnDwP6982Z/umnn3jm2ed46KFhRdH9RSX0iSM4uCx+fv4EBATwyjuf5jz32cyPmPLWy0xfkEy5\n8hXYs2s7r7/kvNNqVtYZ+txxDx2vjwPgieGD+eNYGoGBQQCM+/c7VKjo1j+H1/j6PXcnxM+LSEXg\nUWAyUB54xM31f2gt83HBysu/Ro0a8c3mrQBkZWVRp3YtevS4tai6vygBJrw2hXLlK7i0Hzl8iK3f\nrKda9X/ltIXVu4rX3puBn58fx46mMTThNq6N6Yy/vz8gJD49gQYNi8ddUMH37/klhxPGmIXGmOPG\nmO3GmFhjTEtjzAJ3Vm6MSQH+KHSVBbRixQrq1a9P7dq1Lz1zEbjQmeXvvf4ig+8b4dJWunQQftbN\nyTMyThNcNsQKcPZ6vFtnYfjiPc/rx47JeSxnjDG+/Xx2w6yZM+jXr7+vy3ASYfSIIfj5+9E1vidx\n8T1Zv+YrqlSrTt36Dc+bfc+u7bw6IYnff0slMWmiy3OvjB+Nv38pro25gb7/M6SoXoFbfPGe5zWc\n2MyFf9SQi7QXyDPPjM15HBMTS2xsrEfWm5GRwaJFCxk/YeKlZy4CL73xMZVDq3Li+DFGP3ovV9Sp\ny+xp7zPupXdy5sm9pW7UuClvfTSP/f/3M0mP30+zyNaUDSlH4tPjCa1Sjb9PneKFpBH8d+lCrr8x\n3hcv6Ty+es/z+rFjalEUMGbMWK+s98slS2jZshVVq1b1yvrzq3Kos44KFSvTPvp6dny7md9/S+XB\nwT0BSDtymOFD+vLK29OoWCk0Z7naV9blXzWv4GDqfq5qFEFolWoAlAkOJuaGruzZtaPYhNhX73mJ\nvXjKjBnT6du3n6/LAOD06b85deqk8/Hfp9iyaT0NGzdh2vxVTJn5JVNmfkmVqtWY9N5MKlYK5fff\nUjnjcABw+NBBDh74lVpX1OHMmTOcOO78iuFwZLJx3WrC6l3ls9d1Ll+95+4eFF8gIjIdiAFCRWQ/\nkGSM+dCbfQKcPHmSlStX8M6773m7K7cc/+Mo455y7tDJOuMg9oZutLym/UXn/377VuZ8+gH+/qUI\nCAjgwceSCC4bwum/T5H0+P2ccTjIyjpDZOt2xMXfXlQvI0++fM/1WmwFpNdiK3oFvhabiDQSkZUi\n8r013UxEnvJGkUoVhDtj4veAJzn7y912oHgMNpXCvRAHG2M2ZE8Y5/gj03slKZU/7oT4iIg0yJ4Q\nkZ7Ab94rSan8cWfvxIPAu0C4iBwEfgYGeLUqpfLBnYun7AM6WZev8jPG/OX9spRynztndozB+TOz\nAEbEuYfDGPOsd0tTyj3uDCdOcvZYiTJAd2Cn1ypSKp/cGU68lHtaRF4ElnmtIqXyqSDHTpQFanm6\nEKUKyp0x8Q7ODif8gGqAjodVseHOmLgbzi91AA7gd2OM/tihio08QywiAcBSY0x4EdWjVL7lOSY2\nxjiAPSJyZRHVo1S+uTOcqAx8LyIbsW4+g/MQipu9V5ZS7nMnxE9xdkyczZ4HAasSya0vdsaYx3M3\niMhEYLV3SlIqf9zZT9z5Am03eboQpQoqr+tO3A88ANQXkdzn4pQD1nqqAF+eHlUYdj3FB+Drn4/5\nugSPyms48SmwBJiA87a42ePiv7Lv36FUcZDXdSdOACeAvhebR6nioMRed0JdPjTEyvY0xMr2NMTK\n9jTEyvY0xMr2NMTK9jTEyvY0xMr2NMTK9jTEyvY0xMr2NMTK9jTEyvY0xMr2NMTK9rwaYhGpLSJf\nicj3IrJDRDx2K9277x5MrZo1iIxsltM28vFEmjaJoGXLSHr1vJ0TJ04A8Omn02jdumXOn9KBAXz3\n3XeeKqXA9u/fT6dO19Gs6dU0b9aEyZMnAdCvbx9at2pB61YtaFC/Lq1btfBZjf/8c5p7bu/MoPgY\nBt7YjrdfdF7B7IPXJnLrtU1IiI8lIT6Wr1evBODP43/w0IBb6Ny8Dq88M9JlXSMSejnXE9ee8U8M\nw5HpmQtJefUWYCJSA6hhjNkmIiE4b7Xbwxizy3reZDqyCrTuNSkplA0JISHhTrZtcwZyxfLlXN+p\nE35+fjw56gkAXhg/wWW5HTt20Kvnbeza/UOBX5dVe6GWBzh06BCHDh0iMjKS9PR0oq5pxdzP5tO4\nceOceRITH6NixYqMHu25G1bl9xy703+fIqhMMA6Hgwf63MTQUc+yeV0ywWVD6HvXA+fN+8PO7fz0\nwy5+/mEXj4w5e4vcUyfTCS4bAsBTQwcR3fkmbuzR2+06OjQILdgtwArDGHPIGLPNepwO7AJqemLd\nHaKjqVSpkkvbDZ0759ydPqpNGw6kpp633Izpn9K7dx9PlFBoNWrUIDIyEoCQkBDCwxtz8ODBnOeN\nMcyZPcvnd0YNKhMMgCMzg6ysM5QrXxG48Em+QWWCadaqDYGBgec9lx1gR2YmmZmZVMh1+9/CKLIx\nsYiEAS2ADXnP6RlTP/yQrl27ntc+Z85s+hST2+Xm9ssvv7Bt21batGmT05aSkkL16tWpX7++DyuD\nrKwsBsXHEN82nJZto6nX0Hlpvrn/eY87u3dk/BPD+OvPEy7LXOyTasSgnsS3Dad0UBBtYzp5pL4i\nCbE1lJgDDLe2yF41/oXnCQwMpF+//i7tGzZsoExwMBEREd4uIV/S09Pp07snr7zyGiEhITntM2dM\np+85r8EX/Pz8mLpwNfPW7GDbxnVs+XoNtw5IYPaqrUxduJrQatV5/YWn3VrXy1Pn8Pm6nWRm/MOS\nz6Z7pD6v3tsZQERKAXOBT4wx8899/tlnxuY8jomJJSY2tlD9ffTRVJYsWcKy5SvOe27WzBn0K2Zb\n4czMTHr1vJ3+AwZyS48eOe0Oh4P58+ex6ZstPqzOVUi58rS/rgu7d2yjZdsOOe3xve9g5BD3/7MF\nli5NzI3x7Px2M11vK/y/h7dvUC7AB8BOY8yrF5onacxYj/W39MsvefnfL7Hyv6sICgpyeS4rK4u5\nc+ewanWKx/orLGMM99x9F40jIhg+/GGX51asWEF448bUrOmRrxAFdvzYUfwDAihXvgL/nP6bTWtX\nkfBQIkeP/E5o1eoAJC9bTL1Grp9u546X/z51kpPpf1GlWg0cDgfrvlpGVIfrPFKjt7fE1wIDge9E\nZKvVNsoY82VhVzxwQH+Sk1eTlpZG3bA6JI0Zy/9OnEBGRgZxcV0AaNu2La+//iYAKcnJ1K5dh7Cw\nsMJ27TFr165l2rRPaNasWc5utHHPjycuLo7Zs2bSt4/vPzWOHvmd5xOHkmWyMFlZ3NijN63bx/Dc\nY/ezd9cOEKHmFXVIHPdyzjI9YyI5dTKdzMwMUpZ/wSsfzaV8xUqMum8gGRkZYAxR0dfTrZdnbofo\n1V1sl+y8ELvYfM0Tu9h8xa6XsfLJLjalioKGWNmehljZnoZY2Z6GWNmehljZnoZY2Z6GWNmehljZ\nnoZY2Z6GWNmehljZnoZY2Z6GWNmehljZXokN8epVq3xdQoGtsmntW75e45N+S26IV6/ydQkFZtfa\nt27w2C2/86XEhlhdPjTEyvZ8fo6dzzpXtnShc+x8GmKlPEGHE8r2NMTK9jTEyvZKZIhFJE5EdovI\njyIy8tJLFA8iMkVEfheR7b6uJT+8eTF1t/ovaV/sRMQf2APcAKQCm4B+2Rf2Ls5EJBpIBz42xjT1\ndT3uutTF1L2tJG6Jo4C9xphfjDGZwAzgFh/X5BZjTArwh6/ryC9vXkzdHSUxxLWA/bmmD1htqggU\n9cXUoWSGuGSNj2ykqC+mnq0khjgVqJ1rujbOrbHyoktdTN2bSmKIvwGuEpEwEQkE+gALfFxTiebO\nxdS9qcSF2BjjAB4ElgI7gZl22DMBICLTgXVAQxHZLyIJvq7JTdkXU79ORLZaf+KKqvMSt4tNXX5K\n3JZYXX40xMr2NMTK9jTEyvY0xMr2NMTK9jTEHiYisSKy0Hocn9ehoCJSQUTuL0AfY0XkUXfbz5ln\nqojcno++wor7oaEaYjeJSL7fK2PMQmPMxDxmqQQ8UIByLrZz352d/iXuh4HLPsTWlma3iHwiIjtF\nZLaIlLGe+0VEJojIZqCXiHQRkXUisllEZolIWWu+OBHZZc13a651DxKRydbj6iIyT0S2WX/aAROA\n+tYvXBOt+RJFZKOIfCsiY3Ota7SI7BGRFKCRG6/rHms920RkTvZrstwgIpus9XWz5vcXkRdz9T2k\nkG9tkbnsQ2xpCLxhjIkA/uTs1tEAacaYVsBKYDTQyZreDIwQkSDgXaC71V6DC2/tJgFfGWMigZbA\n98BIYJ8xpoUxZqSIdAEaGGOicB7O2EpEokWkFc5jQJoDNwHXXKSP3OYaY6Ks/nYBd1ntAlxpjLkG\n6Aa8LSKlreePW31HAfdYh1UWe96+Qbld7DfGrLcefwIMA/5tTc+0/m4LRADrrPs6B+I8zqER8LMx\nZl+u5S+0FbsO5/EFGGOygD9FpPI583QBuuS6mXtZ4CqgHPCZMeY0cFpEFuAMY16aisg4oAIQAmTf\nFN4As6w69orIT0C41XdTEelpzVceaADsvUQ/Pqchdsq9VZNzpk/merzcGNM/94Ii0vycdeUVLnfu\naj7eGPPuOX0MP2fZvNaTXftU4GZjzHYRuROIdWOZB40xy8/pO+zSJfuWDiec6ohIW+txfyDlAvNs\nAK4VkfoAIlJWRK4CdgNhIlLPmq/fRfpYCdxvLesvIuWBv3BuZbMtBQbnGmvXEpGqQDLQQ0SCRKQc\n0J2LDyeyAx4CHLKO8x2Ya37BOb4X67XUs17DUuABEQmw+m4oIsEX6aNY0S2x0x5gqIhMwTlWfctq\nzwmKMeaIiAwCpltjSIDRxpgfrS9Bi0XkFM7/AGVzLZ+9juHAuyJyF3AGuM8Ys0FE1lq7sL6wxsWN\ngfXWkOUvYKAxZquIzAS+BQ4DG/N4Ldn9PY3zP94R6++QXM//aq2jPHCvMSZDRN4HwoAt1vHBh4Ee\n574PxdFlfyim9XG50E5nFytXOpxwurz/J9vcZb8lVvanW2JlexpiZXsaYmV7GmJlexpiZXv/DxMo\nwg/o3D2uAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1984b160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from mlxtend.evaluate import plot_confusion_matrix\n",
    "%matplotlib inline\n",
    "\n",
    "cm = confusion_matrix(y_train, \n",
    "                      SVC(kernel = 'linear', probability = True).fit(X_train, y_train).predict(X_train))\n",
    "fig, ax = plot_confusion_matrix(conf_mat = cm)\n",
    "plt.show()\n",
    "\n",
    "#0 : The tweet contains hate speech\n",
    "#1 : The tweet is not offensive\n",
    "#2 : The tweet uses offensive language but not hate speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's incredibly good at classifying non-offensive tweets, but less so with the category of utmost importance, hate speech. This is where the concepts of precision and recall enter the frame. The former is a measure of how well the classifier doesn't mislabel a class will recall measures how well it gets every label for the class. For those tweets that contain hate speech, the precision is around 93% while the recall is about 92%.\n",
    "\n",
    "In this case, the most pertinent metric comes down to what is more important: making sure that tweets without hate speech aren't misclassified or getting as much instances of hate speech being used as possible. It's a trade-off as concentrating on one negatively affects the other. \n",
    "\n",
    "For now I'm going to use the F1 score which is a combination of precision and recall. It also is about the same as the accuracy in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97361442551737953"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_train,\n",
    "         SVC(kernel = 'linear', probability = True).fit(X_train, y_train).predict(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter tuning of the SVC is long. My setup is below. The cross validation ran to completion and did badly on average. I had to stop the grid search lest it continue until the end of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.56382002  0.57024793  0.56382002  0.56473829  0.56473829  0.57208448\n",
      "  0.55555556  0.57090239  0.56169429  0.58471455]\n",
      "0.567231582525\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "scores = cross_val_score(SVC(kernel = 'linear', probability = True), X_train, y_train, cv = 10, scoring = 'accuracy')\n",
    "print(scores)\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "C_range = np.logspace(-2, 4, 5)\n",
    "gamma_range = np.logspace(-3, 3, 5)\n",
    "param_grid = dict(gamma = gamma_range, C = C_range)\n",
    "grid_svc = GridSearchCV(SVC(kernel = 'linear', probability = True), \n",
    "                        param_grid = param_grid, \n",
    "                        cv = 10, \n",
    "                        scoring = 'accuracy')\n",
    "grid_svc.fit(X_train, y_train)\n",
    "grid_svc.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using cross validation with the Random Forest Classifier works much better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.76767677  0.7594123   0.75573921  0.74471993  0.77043159  0.75827206\n",
      "  0.77389706  0.77460902  0.77920883  0.74677716]\n",
      "0.763074392682\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "scores_rfc = cross_val_score(RandomForestClassifier(), X_train, y_train, cv = 10, scoring = 'accuracy')\n",
    "print(scores_rfc)\n",
    "print(np.mean(scores_rfc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clearly the better of the two models. The next step would be to do grid search to make the model even better, but that's something I have to return to. For now here's the skeleton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "param_grid = {\"max_depth\": [3, None],\n",
    "              \"n_estimators\": [10, 50, 100]\n",
    "              \"max_features\": [1, 3, 10],\n",
    "              \"min_samples_split\": [1, 3, 10],\n",
    "              \"min_samples_leaf\": [1, 3, 10],\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "grid_svc = GridSearchCV(RandomForestClassifier(), \n",
    "                        param_grid = param_grid, \n",
    "                        cv = 10, \n",
    "                        scoring = 'accuracy')\n",
    "grid_svc.fit(X_train, y_train)\n",
    "grid_svc.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though it hasn't been optimized, I'm going to built an app to classify hate speech with this model. To end, let export the pickle objects I'll need for the app, and look at how the model performs on the test set and on a constructed tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "pickle.dump(clf_rfc, open('classifier.pkl', 'wb'), protocol = 4)\n",
    "pickle.dump(stopwords.words('english'), open('stopwords.pkl', 'wb'), protocol = 4)\n",
    "pickle.dump(vectorizer, open('vectorizer.pkl', 'wb'), protocol = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76019845644983464"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_rfc = RandomForestClassifier()\n",
    "clf_rfc.fit(X_train, y_train)\n",
    "clf_rfc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The model says: The tweet is not offensive with 42.38% confidence.'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def classify_tweet(tweet, clf):\n",
    "    tweet_to_clf = processTweet(tweet)\n",
    "    tweet_to_clf = vectorizer.transform([tweet_to_clf])\n",
    "    label = clf.predict(tweet_to_clf)[0]\n",
    "    confidence = max(clf.predict_proba(tweet_to_clf)[0])*100\n",
    "    return 'The model says: ' + le.inverse_transform(label) + ' with ' + str(round(confidence, 2)) + '% confidence.'\n",
    "\n",
    "tweet = 'I hate you all.'\n",
    "classify_tweet(tweet, clf_rfc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I suppose it's right. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
